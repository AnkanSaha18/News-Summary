# -*- coding: utf-8 -*-
"""Copy of abstracttextsummarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-e49vSMa_okDQMZM-pAvpDccEuY-8eb0
"""

# from google.colab import drive
# drive.mount('/content/drive')
#
# !pip install --quiet transformers
# !pip install --quiet pytorch-lightning

# import json
# import pandas as pd
# import numpy as np
# import torch
# from pathlib import Path
# from torch.utils.data import Dataset,DataLoader
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger
# from sklearn.model_selection import train_test_split
# from termcolor import colored
# import textwrap
from transformers import (
    AdamW,
    T5ForConditionalGeneration,
    T5TokenizerFast as T5Tokenizer
)

# from tqdm.auto import tqdm
# import seaborn as sns
# from pylab import rcParams
# import matplotlib.pyplot as plt
# from matplotlib import rc
# from pytorch_lightning.callbacks.early_stopping import EarlyStopping
# import torch
# import torch.nn.functional as F
# import torchvision
# import torchvision.transforms as transforms


# %matplotlib inline
# %config InlineBackend.figure_format='retina'
# sns.set(style='whitegrid', palette='muted', font_scale=1.2)
# rcParams['figure.figsize']=16, 10

pl.seed_everything(42)

# !gdown 1jEn-v4kDIYVdQ-XRWowUjhV5IlRP8TSF

# !unzip -q news_summary.csv.zip -d data

# df=pd.read_csv("data/news_summary.csv",encoding="latin-1")
# df.head()

# df=df[["text","ctext"]]
# df.head()

# df.columns=["summary","text"]
# df=df.dropna()
# df.head()

# df.shape

# train_df,test_df=train_test_split(df,test_size=0.1)
# train_df.shape,test_df.shape

# class NewsSummaryDataset(Dataset):
#     def __init__(
#         self,
#         data: pd.DataFrame,
#         tokenizer: T5Tokenizer,
#         text_max_token_len: int = 512,
#         summary_max_token_len: int = 128):

#         self.tokenizer = tokenizer
#         self.data = data
#         self.text_max_token_len = text_max_token_len
#         self.summary_max_token_len = summary_max_token_len
#     def __len__(self):
#         return len(self.data)

#     def __getitem__(self, index:int):
#         data_row = self.data.iloc[index]
#         text = data_row["text"]
#         text_encoding = tokenizer(data_row["text"],max_length=self.text_max_token_len,
#                                  padding="max_length",
#                                  truncation=True,
#                                  return_attention_mask=True,
#                                  add_special_tokens=True,
#                                  return_tensors="pt")

#         summary = data_row["summary"]
#         summary_encoding = tokenizer(summary,max_length=self.summary_max_token_len,
#                                  padding="max_length",
#                                  truncation=True,
#                                  return_attention_mask=True,
#                                  add_special_tokens=True,
#                                  return_tensors="pt")

#         labels= summary_encoding["input_ids"]
#         labels[labels == 0] = -100

#         return dict(
#             text=text,
#             summary=summary,
#              text_input_ids=text_encoding["input_ids"].flatten(),
#             text_attention_mask=text_encoding["attention_mask"].flatten(),
#             labels=labels.flatten(),
#             labels_attention_mask=summary_encoding["attention_mask"].flatten()
#           )

# class NewsSummaryDataModule(pl.LightningDataModule):
#     def __init__(self,
#                 train_df:pd.DataFrame,
#                 test_df:pd.DataFrame,
#                 tokenizer:T5Tokenizer,
#                 batch_size: int = 8,
#                 text_max_token_len: int = 512,
#                 summary_max_token_len: int = 128):
#         super().__init__()
#         self.train_df=train_df
#         self.test_df=test_df

#         self.batch_size=batch_size
#         self.tokenizer=tokenizer
#         self.text_max_token_len=text_max_token_len
#         self.summary_max_token_len= summary_max_token_len

#     def setup(self, stage=None):
#         self.train_dataset =  NewsSummaryDataset(
#             self.train_df,
#             self.tokenizer,
#             self.text_max_token_len,
#             self.summary_max_token_len
#         )
#         self.test_dataset =  NewsSummaryDataset(
#             self.test_df,
#             self.tokenizer,
#             self.text_max_token_len,
#             self.summary_max_token_len
#         )

#     def train_dataloader(self):
#         return DataLoader(
#             self.train_dataset,
#             batch_size=self.batch_size,
#             shuffle=True,
#             num_workers=2)

#     def val_dataloader(self):
#       return DataLoader(
#             self.test_dataset,
#             batch_size=self.batch_size,
#             shuffle=False,
#             num_workers=2)

#     def test_dataloader(self):
#         return DataLoader(
#             self.test_dataset,
#             batch_size=self.batch_size,
#             shuffle=False,
#             num_workers=2)

MODEL_NAME = "t5-base"
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)


# text_token_counts, summary_token_counts = [],[]
# for _, row in train_df.iterrows():
#     text_token_count=len(tokenizer.encode(row['text']))
#     text_token_counts.append(text_token_count)

#     summary_token_count=len(tokenizer.encode(row['summary']))
#     summary_token_counts.append(summary_token_count)

# fig,(ax1,ax2) = plt.subplots(1,2)
# sns.histplot(text_token_counts,ax=ax1)
# ax1.set_title("Full news token count")
# sns.histplot(summary_token_counts,ax=ax2)
# ax2.set_title("Summary news token count")

# N_EPOCHS = 6
# BATCH_SIZE = 8

# data_module = NewsSummaryDataModule(train_df, test_df, tokenizer)

class NewsSummaryModel(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

    def forward(self, inputs_ids, attention_mask, decoder_attention_mask, labels=None):
        output = self.model(inputs_ids,
                            attention_mask=attention_mask,
                            labels=labels,
                            decoder_attention_mask=decoder_attention_mask)
        return output.loss, output.logits

    def step(self, batch, batch_idx):
        input_ids = batch["text_input_ids"]
        attention_mask = batch["text_attention_mask"]
        labels = batch["labels"]
        labels_attention_mask = batch["labels_attention_mask"]
        loss, outputs = self.forward(inputs_ids=input_ids,
                                     attention_mask=attention_mask,
                                     decoder_attention_mask=labels_attention_mask,
                                     labels=labels)
        return loss, outputs

    def training_step(self, batch, batch_idx):
        loss, outputs = self.step(batch, batch_idx)

        self.log("train_loss", loss, prog_bar=True, logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        loss, outputs = self.step(batch, batch_idx)
        self.log("val_loss", loss, prog_bar=True, logger=True)
        return loss

    def test_step(self, batch, batch_idx):
        loss, outputs = self.step(batch, batch_idx)
        self.log("test_loss", loss, prog_bar=True, logger=True)
        return loss

    def configure_optimizers(self):
        return AdamW(self.parameters(), lr=0.0001)


model = NewsSummaryModel()

# %load_ext tensorboard
# %tensorboard --logdir ./lightning_logs

# checkpoint_callback=ModelCheckpoint(
#     dirpath="checkpoints",
#     filename="best_checkpoint",
#     save_top_k=1,
#     verbose=True,
#     monitor="val_loss",
#     mode="min"
# )

# path = "/content/drive/MyDrive/summary"
# logger = TensorBoardLogger("lightning_logs",name="news-summary")

# trainer = pl.Trainer(default_root_dir="/content/drive/MyDrive/summarization_trained_model",
#                     enable_checkpointing=checkpoint_callback,
#                     max_epochs=N_EPOCHS,
#                     gpus=1
#                    )

# trainer.fit(model, data_module)

# path = "/content/drive/MyDrive/summarization"
# from transformers import AutoModel

# trained_model = AutoModel.from_pretrained(path)

# ckpt = "/content/drive/MyDrive/summarization_trained_model/lightning_logs/version_0/checkpoints/epoch=5-step=2970.ckpt"
ckpt = "venv/lib/python3.10/site-packages/epoch=5-step=2970.ckpt"

trained_model = NewsSummaryModel.load_from_checkpoint(ckpt)
trained_model.eval()


def abstractive_summary(text):
    text_encoding = tokenizer(
        text,
        max_length=512,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        add_special_tokens=True,
        return_tensors="pt"
    )
    generated_ids = trained_model.model.generate(
        input_ids=text_encoding["input_ids"],
        attention_mask=text_encoding["attention_mask"],
        max_length=150,
        num_beams=2,
        repetition_penalty=2.5,
        length_penalty=1.0,
        early_stopping=True
    )
    preds = [
        tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenizer_spaces=True)
        for gen_id in generated_ids
    ]
    return "".join(preds)


# sample_row=test_df.iloc[0]
# news=sample_row["text"]
# org_summary=sample_row["summary"]
# model_summary=summarize(news)

# print('Original news :  \n',news)
# print('\nOriginal summary :  \n',org_summary)
# print('\nModel summary :  \n',model_summary)

# sample_row=test_df.iloc[66]
# news=sample_row["text"]
# org_summary=sample_row["summary"]
# model_summary=summarize(news)
# print('Original text :  \n',news)
# print('\nOriginal summary :  \n', org_summary)
# print('\nModel summary :  \n', model_summary)







# news = "The Russian-backed leader in Ukraine’s Kherson region announced Tuesday that there would be a further “organized relocation” of civilians away from frontline settlements.“I took the difficult but correct decision to announce the organized relocation of the civilian population of Beryslav, Bilozerka, Snihurivka and Oleksandrivka communities to the left bank of the Dnipro river,” Vladimir Saldo said on Telegram, referring to the eastern bank of the river.“This decision was prompted by the creation of large-scale defensive fortifications so that any attack could be repelled. There is no place for civilians where the military operate. Let the Russian army do its job.”Saldo said that any civilians who decided to move on “to the regions of Russia” would be given assistance with housing.Some background: The newly appointed commander of Russia’s “special military operation” in Ukraine, Gen. Sergey Surovikin, on Tuesday called the situation in the Kherson region “very difficult.”“The Russian army will ensure the safe evacuation of the population,” Surovikin said.Ukraine has made significant advances toward Kherson in recent weeks, along the western (or right) bank of the Dnipro river. The head of Ukraine’s Defense Intelligence Agency said that he hoped to recapture the city by the end of the year.The Ukrainian military said Monday that Russian forces were busy building fortifications in the Kherson region and that they were moving civilians to Crimea.The deputy Ukrainian head of the Kherson region characterized Russia’s “evacuations” as semi-voluntary deportation of the Ukrainian population."
# summary = abstractive_summary(news)
# print(summary)
#
# print(len(news))
# print(len(summary))
# print((len(summary) / len(news)) * 100, "%")

